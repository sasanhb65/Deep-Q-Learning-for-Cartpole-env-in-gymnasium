{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sasanhb65/Deep-Q-Learning-for-Cartpole-env-in-gymnasium/blob/main/2.0.1%20-%20Reinforcement%20Learning%20-%20Keras%20DQN%20Atari.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxVtMbX6_ZVJ"
      },
      "source": [
        " <a href=\"https://colab.research.google.com/github/ypeleg/keras_rl_tutorial/blob/master/2.0.1%20-%20Reinforcement%20Learning%20-%20Keras%20DQN%20Atari.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJEzhGMI_ZVL"
      },
      "source": [
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sUHWRAr_ZVL"
      },
      "source": [
        "<div>\n",
        "    <center><strong><h5>Reinforcement Learning Tutorial!</h5></strong></center>\n",
        "    <center><strong><h2>2.0.1 Keras RL - A Standard interface for practical RL. </h2></strong></center>\n",
        "<div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdUsswHM_ZVM"
      },
      "source": [
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FeqOMhx_ZVM"
      },
      "source": [
        "<table>\n",
        "  <tr>\n",
        "    <td><img src=\"https://github.com/ypeleg/keras_rl_tutorial/blob/master/img/breakout.gif?raw=1\" width=\"200\"></td>\n",
        "    <td><img src=\"https://github.com/ypeleg/keras_rl_tutorial/blob/master/img/cartpole.gif?raw=1\" width=\"200\"></td>\n",
        "    <td><img src=\"https://github.com/ypeleg/keras_rl_tutorial/blob/master/img/pendulum.gif?raw=1\" width=\"200\"></td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9BtoExI_ZVN"
      },
      "source": [
        "## What is it?\n",
        "\n",
        "`keras-rl` implements some state-of-the art deep reinforcement learning algorithms in Python and seamlessly integrates with the deep learning library [Keras](http://keras.io).\n",
        "\n",
        "Furthermore, `keras-rl` works with [OpenAI Gym](https://gym.openai.com/) out of the box. This means that evaluating and playing around with different algorithms is easy.\n",
        "\n",
        "Of course you can extend `keras-rl` according to your own needs. You can use built-in Keras callbacks and metrics or define your own.\n",
        "Even more so, it is easy to implement your own environments and even algorithms by simply extending some simple abstract classes. Documentation is available [online](http://keras-rl.readthedocs.org)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ost4RoWh_ZVN"
      },
      "source": [
        "## TL;DR\n",
        "\n",
        "---\n",
        "\n",
        "### DQNAgent\n",
        "\n",
        "```python\n",
        "rl.agents.dqn.DQNAgent(model, policy=None, test_policy=None, enable_double_dqn=True,\n",
        "                       enable_dueling_network=False, dueling_type='avg')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxdKU20q_ZVO"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8jAcZfh_ZVO"
      },
      "source": [
        "<span style=\"float:right;\">[[source]](https://github.com/keras-rl/keras-rl/blob/master/rl/core.py#L11)</span>\n",
        "### Agent\n",
        "\n",
        "```python\n",
        "rl.core.Agent(processor=None)\n",
        "```\n",
        "\n",
        "Abstract base class for all implemented agents.\n",
        "\n",
        "Each agent interacts with the environment (as defined by the `Env` class) by first observing the\n",
        "state of the environment. Based on this observation the agent changes the environment by performing\n",
        "an action.\n",
        "\n",
        "Do not use this abstract base class directly but instead use one of the concrete agents implemented.\n",
        "Each agent realizes a reinforcement learning algorithm. Since all agents conform to the same\n",
        "interface, you can use them interchangeably.\n",
        "\n",
        "All agents share a common API. This allows you to easily switch between different agents.\n",
        "That being said, keep in mind that some agents make assumptions regarding the action space, i.e. assume discrete\n",
        "or continuous actions.\n",
        "\n",
        "<span style=\"float:right;\">[[source]](https://github.com/keras-rl/keras-rl/blob/master/rl/core.py#L44)</span>\n",
        "\n",
        "### fit\n",
        "\n",
        "\n",
        "```python\n",
        "fit(self, env, nb_steps, action_repetition=1, callbacks=None, verbose=1, visualize=False, nb_max_start_steps=0, start_step_policy=None, log_interval=10000, nb_max_episode_steps=None)\n",
        "```\n",
        "\n",
        "\n",
        "Trains the agent on the given environment.\n",
        "\n",
        "__Arguments__\n",
        "\n",
        "- __env:__ (`Env` instance): Environment that the agent interacts with. See [Env](#env) for details.\n",
        "- __nb_steps__ (integer): Number of training steps to be performed.\n",
        "- __action_repetition__ (integer): Number of times the agent repeats the same action without\n",
        "\tobserving the environment again. Setting this to a value > 1 can be useful\n",
        "\tif a single action only has a very small effect on the environment.\n",
        "- __callbacks__ (list of `keras.callbacks.Callback` or `rl.callbacks.Callback` instances):\n",
        "\tList of callbacks to apply during training. See [callbacks](/callbacks) for details.\n",
        "- __verbose__ (integer): 0 for no logging, 1 for interval logging (compare `log_interval`), 2 for episode logging\n",
        "- __visualize__ (boolean): If `True`, the environment is visualized during training. However,\n",
        "\tthis is likely going to slow down training significantly and is thus intended to be\n",
        "\ta debugging instrument.\n",
        "- __nb_max_start_steps__ (integer): Number of maximum steps that the agent performs at the beginning\n",
        "\tof each episode using `start_step_policy`. Notice that this is an upper limit since\n",
        "\tthe exact number of steps to be performed is sampled uniformly from [0, max_start_steps]\n",
        "\tat the beginning of each episode.\n",
        "- __start_step_policy__ (`lambda observation: action`): The policy\n",
        "\tto follow if `nb_max_start_steps` > 0. If set to `None`, a random action is performed.\n",
        "- __log_interval__ (integer): If `verbose` = 1, the number of steps that are considered to be an interval.\n",
        "- __nb_max_episode_steps__ (integer): Number of steps per episode that the agent performs before\n",
        "\tautomatically resetting the environment. Set to `None` if each episode should run\n",
        "\t(potentially indefinitely) until the environment signals a terminal state.\n",
        "\n",
        "__Returns__\n",
        "\n",
        "A `keras.callbacks.History` instance that recorded the entire training process.\n",
        "\n",
        "----\n",
        "\n",
        "<span style=\"float:right;\">[[source]](https://github.com/keras-rl/keras-rl/blob/master/rl/core.py#L231)</span>\n",
        "\n",
        "### test\n",
        "\n",
        "\n",
        "```python\n",
        "test(self, env, nb_episodes=1, action_repetition=1, callbacks=None, visualize=True, nb_max_episode_steps=None, nb_max_start_steps=0, start_step_policy=None, verbose=1)\n",
        "```\n",
        "\n",
        "\n",
        "- __processor__ (`Processor` instance): See [Processor](#processor) for details.\n",
        "\n",
        "----\n",
        "\n",
        "<span style=\"float:right;\">[[source]](https://github.com/keras-rl/keras-rl/blob/master/rl/core.py#L454)</span>\n",
        "### Processor\n",
        "\n",
        "```python\n",
        "rl.core.Processor()\n",
        "```\n",
        "\n",
        "Abstract base class for implementing processors.\n",
        "\n",
        "A processor acts as a coupling mechanism between an `Agent` and its `Env`. This can\n",
        "be necessary if your agent has different requirements with respect to the form of the\n",
        "observations, actions, and rewards of the environment. By implementing a custom processor,\n",
        "you can effectively translate between the two without having to change the underlaying\n",
        "implementation of the agent or environment.\n",
        "\n",
        "Do not use this abstract base class directly but instead use one of the concrete implementations\n",
        "or write your own.\n",
        "\n",
        "----\n",
        "\n",
        "<span style=\"float:right;\">[[source]](https://github.com/keras-rl/keras-rl/blob/master/rl/core.py#L533)</span>\n",
        "### Env\n",
        "\n",
        "```python\n",
        "rl.core.Env()\n",
        "```\n",
        "\n",
        "The abstract environment class that is used by all agents. This class has the exact\n",
        "same API that OpenAI Gym uses so that integrating with it is trivial. In contrast to the\n",
        "OpenAI Gym implementation, this class only defines the abstract methods without any actual\n",
        "implementation.\n",
        "\n",
        "----\n",
        "\n",
        "<span style=\"float:right;\">[[source]](https://github.com/keras-rl/keras-rl/blob/master/rl/core.py#L609)</span>\n",
        "### Space\n",
        "\n",
        "```python\n",
        "rl.core.Space()\n",
        "```\n",
        "\n",
        "Abstract model for a space that is used for the state and action spaces. This class has the\n",
        "exact same API that OpenAI Gym uses so that integrating with it is trivial.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lOBizVa4rVt"
      },
      "source": [
        "### Installations"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68776c28-b370-4a22-c4ef-1cc752371a86",
        "id": "yH1ObOau_ZVQ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.81)] [Connecting to security.ubuntu.com] [Connecti\r                                                                                                    \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.81)] [Connecting to security.ubuntu.com] [Connecte\r                                                                                                    \rHit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadcont\r                                                                                                    \rHit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadcontent.net (185.125.190.8\r                                                                                                    \r0% [Waiting for headers] [Waiting for headers] [Waiting for headers]\r                                                                    \rHit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.12).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 27 not upgraded.\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n",
            "--2025-02-07 15:59:04--  https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/xvfb\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 640 [text/plain]\n",
            "Saving to: ‘../xvfb’\n",
            "\n",
            "../xvfb             100%[===================>]     640  --.-KB/s    in 0s      \n",
            "\n",
            "2025-02-07 15:59:04 (42.4 MB/s) - ‘../xvfb’ saved [640/640]\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "E: Unable to locate package python-opengl\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "!apt-get update && apt-get install -y xvfb  --fix-missing\n",
        "!pip install gym\n",
        "!wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/xvfb -O ../xvfb\n",
        "!apt-get install -y python-opengl ffmpeg\n",
        "import os\n",
        "!bash ../xvfb start\n",
        "os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "872812c7-9cee-4c72-ae3d-f852c29b5c26",
        "id": "ncQarK3M_ZVR"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
            "Collecting keras-rl2\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl.metadata (304 bytes)\n",
            "Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras-rl2\n",
            "Successfully installed keras-rl2-1.0.5\n",
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Collecting ale-py>=0.9 (from gymnasium[atari])\n",
            "  Downloading ale_py-0.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Downloading ale_py-0.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ale-py\n",
            "  Attempting uninstall: ale-py\n",
            "    Found existing installation: ale-py 0.8.1\n",
            "    Uninstalling ale-py-0.8.1:\n",
            "      Successfully uninstalled ale-py-0.8.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ale-py-0.10.1\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install keras-rl2 --no-deps\n",
        "!pip install 'gymnasium[atari]'\n",
        "print('Done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_Gat9kk_ZVR"
      },
      "source": [
        "## Q-Learning with Neural Networks\n",
        "\n",
        "Now, you may be thinking: tables are great, but they don’t really scale, do they? While it is easy to have a 16x4 table for a simple grid world, the number of possible states in any modern game or real-world environment is nearly infinitely larger. For most interesting problems, tables simply don’t work.\n",
        "\n",
        "We instead need some way to take a description of our state, and produce $Q$-values for actions without a table: that is where neural networks come in. By acting as a function approximator, we can take any number of possible states that can be represented as a vector and learn to map them to $Q$-values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTl7j809_ZVR"
      },
      "source": [
        "In the case of the FrozenLake example, we will be using a one-layer network which takes the state encoded in a one-hot vector (1x16), and produces a vector of 4 $Q$-values, one for each action.\n",
        "\n",
        "Such a simple network acts kind of like a glorified table, with the network weights serving as the old cells. The key difference is that we can easily expand the Tensorflow network with added layers, activation functions, and different input types, whereas all that is impossible with a regular table.\n",
        "\n",
        "<img src=\"https://github.com/ypeleg/keras_rl_tutorial/blob/master/images/pong.jpg?raw=1\" alt=\"\" style=\"width: 300px;\"/>\n",
        "\n",
        "The method of updating is a little different as well.\n",
        "\n",
        "Instead of directly updating our table, with a network we will be using backpropagation and a loss function. Our loss function will be sum-of-squares loss, where the difference between the current predicted $Q$-values, and the “target” value is computed and the gradients passed through the network.\n",
        "\n",
        "In this case, our $Q_{target}$ for the chosen action is the equivalent to the $Q$-value computed in equation above ($\n",
        "Q(s,a) + \\alpha [r + \\gamma max_{a'} Q(s',a') - Q(s,a) ]\n",
        "$).\n",
        "\n",
        "$$\n",
        "Loss = \\sum (Q_{target} - Q_{predicted})^2\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1Bkcpl31_ZVS",
        "outputId": "040c59a7-0b8f-429e-efe8-0f321c619b1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras==2.3.1 in /usr/local/lib/python3.11/dist-packages (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from keras==2.3.1) (1.26.4)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.11/dist-packages (from keras==2.3.1) (1.13.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from keras==2.3.1) (1.17.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from keras==2.3.1) (6.0.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras==2.3.1) (3.12.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from keras==2.3.1) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from keras==2.3.1) (1.1.2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RecursionError",
          "evalue": "maximum recursion depth exceeded in comparison",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-dc27ad4188cc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConvolution2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPermute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlayer_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_source_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlayer_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvis_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvis_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnp_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/engine/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_source_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/metrics.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1848\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m'2.0.0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1850\u001b[0;31m         \u001b[0mBaseMeanIoU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMeanIoU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfll_initialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfll_keras_version\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"keras_3\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m       if (\n\u001b[1;32m    184\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfll_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"v1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "... last 1 frames repeated, from the frame below ...\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfll_initialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfll_keras_version\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"keras_3\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m       if (\n\u001b[1;32m    184\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfll_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"v1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded in comparison"
          ]
        }
      ],
      "source": [
        "!pip install keras==2.3.1\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "from IPython import display\n",
        "from PIL import Image\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute, Input\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from rl.callbacks import Callback\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiZl0awW_ZVS"
      },
      "source": [
        "Get the environment and extract the number of actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1j0W7Oq0_ZVS"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = 'CartPole-v0'\n",
        "\n",
        "env = gym.make(ENV_NAME)\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlZDsTqD_ZVS"
      },
      "source": [
        "## Deep Q-networks\n",
        "\n",
        "While our ordinary Q-network was able to barely perform as well as the Q-Table in a simple game environment, Deep $Q$-Networks are much more capable. In order to transform an ordinary Q-Network into a DQN we will be making the following improvements:\n",
        "+ Going from a single-layer network to a multi-layer convolutional network.\n",
        "+ Implementing Experience Replay, which will allow our network to train itself using stored memories from it’s experience.\n",
        "+ Utilizing a second “target” network, which we will use to compute target $Q$-values during our updates.\n",
        "\n",
        "<img src=\"https://github.com/ypeleg/keras_rl_tutorial/blob/master/images/deepq1.png?raw=1\" alt=\"\" style=\"width: 800px;\"/>\n",
        "\n",
        "\n",
        "See https://jaromiru.com/2016/09/27/lets-make-a-dqn-theory/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_92bPrKy_ZVT"
      },
      "source": [
        "### Convolutional Layers\n",
        "\n",
        "Since our agent is going to be learning to play video games, it has to be able to make sense of the game’s screen output in a way that is at least similar to how humans or other intelligent animals are able to. Instead of considering each pixel independently, convolutional layers allow us to consider regions of an image, and maintain spatial relationships between the objects on the screen as we send information up to higher levels of the network.\n",
        "\n",
        "### Experience Replay\n",
        "\n",
        "The second major addition to make DQNs work is Experience Replay.\n",
        "\n",
        "The problem with online learning is that the *samples arrive in order* they are experienced and as such are highly correlated. Because of this, our network will most likely overfit and fail to generalize properly.\n",
        "\n",
        "The key idea of **experience replay** is that we store these transitions in our memory and during each learning step, sample a random batch and perform a gradient descend on it.\n",
        "\n",
        "The Experience Replay buffer stores a fixed number of recent memories, and as new ones come in, old ones are removed. When the time comes to train, we simply draw a uniform batch of random memories from the buffer, and train our network with them.\n",
        "\n",
        "### Separate Target Network\n",
        "\n",
        "This second network is used to generate the $Q$-target values that will be used to compute the loss for every action during training.\n",
        "\n",
        "The issue is that at every step of training, the $Q$-network’s values shift, and if we are using a constantly shifting set of values to adjust our network values, then the value estimations can easily spiral out of control. The network can become destabilized by falling into feedback loops between the target and estimated $Q$-values. In order to mitigate that risk, the target network’s weights are fixed, and only periodically or slowly updated to the primary $Q$-networks values. In this way training can proceed in a more stable manner.\n",
        "\n",
        "Instead of updating the target network periodically and all at once, we will be updating it frequently, but slowly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqIq0vjc_ZVT"
      },
      "source": [
        "While the DQN we have described above could learn ATARI games with enough training, getting the network to perform well on those games takes at least a day of training on a powerful machine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3znIjGm9_ZVT"
      },
      "source": [
        "Next, we build a very simple model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJeecBrw_ZVT",
        "outputId": "8aec257c-8471-4a9e-9b39-8b5352b56200"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 1, 4)              0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 16)                80        \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 2)                 34        \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 658\n",
            "Trainable params: 658\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "inp = Input(shape=(1,) + env.observation_space.shape )\n",
        "x = Flatten()(inp)\n",
        "x = Dense(16)(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dense(16)(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dense(16)(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dense(nb_actions)(x)\n",
        "x = Activation('linear')(x)\n",
        "model = Model(inputs=inp, outputs=x)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGBwkJxf_ZVU"
      },
      "source": [
        "Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
        "even the metrics!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dS8vSsw4_ZVU"
      },
      "outputs": [],
      "source": [
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "policy = BoltzmannQPolicy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_575vYga_ZVU"
      },
      "outputs": [],
      "source": [
        "#single\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
        "               target_model_update=1e-2, policy=policy)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpB3sCXw_ZVV"
      },
      "outputs": [],
      "source": [
        "#dual\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
        "               enable_dueling_network=True, dueling_type='avg', target_model_update=1e-2, policy=policy)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl2MQqkB_ZVV"
      },
      "source": [
        "# Renderer\n",
        "for live animation in jupyter while running those command.\n",
        "in case you are running on your local device you can comment the bottom lines and and run full local environment! (It looks nicer :) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8hFpqT7_ZVV"
      },
      "outputs": [],
      "source": [
        "class Render(Callback):\n",
        "    def on_step_end(self, step, logs={}):\n",
        "        plt.clf()\n",
        "        plt.imshow(env.render(mode='rgb_array'))\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OSwI5HD_ZVV"
      },
      "source": [
        "Okay, now it's time to learn something! We visualize the training here for show, but this\n",
        "slows down training quite a lot. You can always safely abort the training prematurely using\n",
        "Ctrl + C."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbMKJh5x_ZVW",
        "outputId": "1c9a0c90-3413-406b-c9f5-8d08410cbea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   72/10000 [..............................] - ETA: 26:53 - reward: 1.0000done, took 11.757 seconds\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc3c0301850>"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAEllJREFUeJzt3XGMnVd95vHvs3ZIKLB1QqaWazvrtPUuSqviZGdDIlCVJqJNsts6lVqUtCoRijSsFCRQ0W6TrrQlUiO1UktatLtR3SZgKpaQDdC4UVqamkgVf5AwAWPsmJQBjGyvEw+QBFi02XX49Y85htth7Lkzd67Hc/r9SFf3fc973nt/J7l65vWZ98xNVSFJ6s+/WO0CJEnjYcBLUqcMeEnqlAEvSZ0y4CWpUwa8JHVqbAGf5PokzySZSXLHuN5HkrSwjOM++CTrgH8A3gwcBT4D3FJVT6/4m0mSFjSuK/grgZmq+kpV/T/gAWDnmN5LkrSA9WN63c3AkYH9o8AbTtf54osvrm3bto2pFElaew4fPszXv/71jPIa4wr4RSWZAqYALrnkEqanp1erFEk650xOTo78GuOaojkGbB3Y39Lavq+qdlXVZFVNTkxMjKkMSfrna1wB/xlge5JLk7wCuBnYM6b3kiQtYCxTNFV1Msk7gE8A64D7q+rgON5LkrSwsc3BV9WjwKPjen1J0pm5klWSOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6ZcBLUqdG+sq+JIeBbwMvAyerajLJRcBHgG3AYeAtVfX8aGVKkpZqJa7gf76qdlTVZNu/A9hbVduBvW1fknSWjWOKZiewu23vBm4aw3tIkhYxasAX8LdJnkoy1do2VtXxtv0ssHHE95AkLcNIc/DAm6rqWJIfAx5L8sXBg1VVSWqhE9sPhCmASy65ZMQyJEnzjXQFX1XH2vMJ4OPAlcBzSTYBtOcTpzl3V1VNVtXkxMTEKGVIkhaw7IBP8qokrzm1DfwCcADYA9zaut0KPDxqkZKkpRtlimYj8PEkp17nf1bV3yT5DPBgktuArwFvGb1MSdJSLTvgq+orwOsXaP8GcN0oRUmSRudKVknqlAEvSZ0y4CWpUwa8JHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTiwZ8kvuTnEhyYKDtoiSPJflSe76wtSfJ+5LMJNmf5IpxFi9JOr1hruA/AFw/r+0OYG9VbQf2tn2AG4Dt7TEF3LsyZUqSlmrRgK+qvwe+Oa95J7C7be8Gbhpo/2DN+TSwIcmmlSpWkjS85c7Bb6yq4237WWBj294MHBnod7S1/ZAkU0mmk0zPzs4uswxJ0umM/EvWqiqglnHerqqarKrJiYmJUcuQJM2z3IB/7tTUS3s+0dqPAVsH+m1pbZKks2y5Ab8HuLVt3wo8PND+1nY3zVXAiwNTOZKks2j9Yh2SfBi4Brg4yVHgd4HfBx5MchvwNeAtrfujwI3ADPBd4G1jqFmSNIRFA76qbjnNoesW6FvA7aMWJUkanStZJalTBrwkdcqAl6ROGfCS1CkDXpI6ZcBLUqcMeEnqlAEvSZ0y4CWpUwa8JHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1atGAT3J/khNJDgy0vSfJsST72uPGgWN3JplJ8kySXxxX4ZKkMxvmCv4DwPULtN9TVTva41GAJJcBNwM/3c75H0nWrVSxkqThLRrwVfX3wDeHfL2dwANV9VJVfRWYAa4coT5J0jKNMgf/jiT72xTOha1tM3BkoM/R1vZDkkwlmU4yPTs7O0IZkqSFLDfg7wV+EtgBHAf+aKkvUFW7qmqyqiYnJiaWWYYk6XSWFfBV9VxVvVxV3wP+jB9MwxwDtg503dLaJEln2bICPsmmgd1fAU7dYbMHuDnJ+UkuBbYDT45WoiRpOdYv1iHJh4FrgIuTHAV+F7gmyQ6ggMPA2wGq6mCSB4GngZPA7VX18nhKlySdyaIBX1W3LNB83xn63w3cPUpRkqTRuZJVkjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdWrR2ySlf+6e2vX2Bdv/7dSfnuVKpKXxCl5ahEGutcqAl6ROGfCS1CkDXpI6ZcBLUqcMeEnqlAEvSZ0y4KVlOt398dK5woCXpE4Z8JLUKQNekjq1aMAn2Zrk8SRPJzmY5J2t/aIkjyX5Unu+sLUnyfuSzCTZn+SKcQ9CkvTDhrmCPwm8u6ouA64Cbk9yGXAHsLeqtgN72z7ADcD29pgC7l3xqiVJi1o04KvqeFV9tm1/GzgEbAZ2Artbt93ATW17J/DBmvNpYEOSTSteuSTpjJY0B59kG3A58ASwsaqOt0PPAhvb9mbgyMBpR1vb/NeaSjKdZHp2dnaJZUuSFjN0wCd5NfBR4F1V9a3BY1VVQC3ljatqV1VNVtXkxMTEUk6Vzjr/ZLDWoqECPsl5zIX7h6rqY635uVNTL+35RGs/BmwdOH1La5MknUXD3EUT4D7gUFW9d+DQHuDWtn0r8PBA+1vb3TRXAS8OTOVIks6SYb6y743AbwJfSLKvtf0O8PvAg0luA74GvKUdexS4EZgBvgu8bUUrliQNZdGAr6pPATnN4esW6F/A7SPWJUkakStZJalTBrwkdcqAl4a00K2S/slgncsMeEnqlAEvSZ0y4CWpUwa8JHXKgJekThnwktQpA14akbdK6lxlwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROGfDSEvjl21pLDHhJ6tQwX7q9NcnjSZ5OcjDJO1v7e5IcS7KvPW4cOOfOJDNJnknyi+McgCRpYcN86fZJ4N1V9dkkrwGeSvJYO3ZPVf3hYOcklwE3Az8N/Djwd0n+dVW9vJKFS5LObNEr+Ko6XlWfbdvfBg4Bm89wyk7ggap6qaq+CswAV65EsZKk4S1pDj7JNuBy4InW9I4k+5Pcn+TC1rYZODJw2lHO/ANBkjQGQwd8klcDHwXeVVXfAu4FfhLYARwH/mgpb5xkKsl0kunZ2dmlnCpJGsJQAZ/kPObC/UNV9TGAqnquql6uqu8Bf8YPpmGOAVsHTt/S2v6JqtpVVZNVNTkxMTHKGKRV51+U1LlomLtoAtwHHKqq9w60bxro9ivAgba9B7g5yflJLgW2A0+uXMmSpGEMcxfNG4HfBL6QZF9r+x3gliQ7gAIOA28HqKqDSR4EnmbuDpzbvYNGks6+RQO+qj4FZIFDj57hnLuBu0eoS5I0IleySlKnDHhJ6pQBL0mdMuAlqVMGvLRE/slgrRUGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS+tEP9ksM41BrwkdcqAl5okQz/G+RrSSjHgJalTw3zhh6QF/NX/nvon+7/047tWqRJpYV7BSytkfuBLq82Al5bBMNdaMMyXbl+Q5Mkkn09yMMldrf3SJE8kmUnykSSvaO3nt/2ZdnzbeIcgnX133TW52iVIixrmCv4l4Nqqej2wA7g+yVXAHwD3VNVPAc8Dt7X+twHPt/Z7Wj+pe87B61wzzJduF/CdtnteexRwLfDrrX038B7gXmBn2wZ4CPhvSdJeR+rG/Kv4u1apDul0hpqDT7IuyT7gBPAY8GXghao62bocBTa37c3AEYB2/EXgtStZtCRpcUMFfFW9XFU7gC3AlcDrRn3jJFNJppNMz87OjvpykqR5lnQXTVW9ADwOXA1sSHJqimcLcKxtHwO2ArTjPwp8Y4HX2lVVk1U1OTExsczyJUmnM8xdNBNJNrTtVwJvBg4xF/S/2rrdCjzctve0fdrxTzr/Lkln3zArWTcBu5OsY+4HwoNV9UiSp4EHkvwe8Dngvtb/PuAvkswA3wRuHkPdkqRFDHMXzX7g8gXav8LcfPz89v8L/NqKVCdJWjZXskpSpwx4SeqUAS9JnfLPBUuNN3upN17BS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6RODfOl2xckeTLJ55McTHJXa/9Akq8m2dceO1p7krwvyUyS/UmuGPcgJEk/bJi/B/8ScG1VfSfJecCnkvx1O/afquqhef1vALa3xxuAe9uzJOksWvQKvuZ8p+2e1x5n+maEncAH23mfBjYk2TR6qZKkpRhqDj7JuiT7gBPAY1X1RDt0d5uGuSfJ+a1tM3Bk4PSjrU2SdBYNFfBV9XJV7QC2AFcm+RngTuB1wL8DLgJ+eylvnGQqyXSS6dnZ2SWWLUlazJLuoqmqF4DHgeur6nibhnkJeD9wZet2DNg6cNqW1jb/tXZV1WRVTU5MTCyveknSaQ1zF81Ekg1t+5XAm4EvnppXTxLgJuBAO2UP8NZ2N81VwItVdXws1UuSTmuYu2g2AbuTrGPuB8KDVfVIkk8mmQAC7AP+Y+v/KHAjMAN8F3jbypctSVrMogFfVfuByxdov/Y0/Qu4ffTSJEmjcCWrJHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROGfCS1KmhAz7JuiSfS/JI2780yRNJZpJ8JMkrWvv5bX+mHd82ntIlSWeylCv4dwKHBvb/ALinqn4KeB64rbXfBjzf2u9p/SRJZ9lQAZ9kC/DvgT9v+wGuBR5qXXYDN7XtnW2fdvy61l+SdBatH7LfHwP/GXhN238t8EJVnWz7R4HNbXszcASgqk4mebH1//rgCyaZAqba7ktJDixrBOe+i5k39k70Oi7od2yOa235V0mmqmrXcl9g0YBP8h+AE1X1VJJrlvtG87Wid7X3mK6qyZV67XNJr2PrdVzQ79gc19qTZJqWk8sxzBX8G4FfTnIjcAHwL4E/ATYkWd+u4rcAx1r/Y8BW4GiS9cCPAt9YboGSpOVZdA6+qu6sqi1VtQ24GfhkVf0G8Djwq63brcDDbXtP26cd/2RV1YpWLUla1Cj3wf828FtJZpibY7+vtd8HvLa1/xZwxxCvtex/gqwBvY6t13FBv2NzXGvPSGOLF9eS1CdXskpSp1Y94JNcn+SZtvJ1mOmcc0qS+5OcGLzNM8lFSR5L8qX2fGFrT5L3tbHuT3LF6lV+Zkm2Jnk8ydNJDiZ5Z2tf02NLckGSJ5N8vo3rrtbexcrsXlecJzmc5AtJ9rU7S9b8ZxEgyYYkDyX5YpJDSa5eyXGtasAnWQf8d+AG4DLgliSXrWZNy/AB4Pp5bXcAe6tqO7CXH/we4gZge3tMAfeepRqX4yTw7qq6DLgKuL39v1nrY3sJuLaqXg/sAK5PchX9rMzuecX5z1fVjoFbItf6ZxHm7kj8m6p6HfB65v7frdy4qmrVHsDVwCcG9u8E7lzNmpY5jm3AgYH9Z4BNbXsT8Ezb/lPgloX6nesP5u6SenNPYwN+BPgs8AbmFsqsb+3f/1wCnwCubtvrW7+sdu2nGc+WFgjXAo8A6WFcrcbDwMXz2tb0Z5G5W8i/Ov+/+0qOa7WnaL6/6rUZXBG7lm2squNt+1lgY9tek+Nt/3y/HHiCDsbWpjH2ASeAx4AvM+TKbODUyuxz0akV599r+0OvOOfcHhdAAX+b5Km2Ch7W/mfxUmAWeH+bVvvzJK9iBce12gHfvZr7Ubtmb1VK8mrgo8C7qupbg8fW6tiq6uWq2sHcFe+VwOtWuaSRZWDF+WrXMiZvqqormJumuD3Jzw0eXKOfxfXAFcC9VXU58H+Yd1v5qONa7YA/ter1lMEVsWvZc0k2AbTnE619TY03yXnMhfuHqupjrbmLsQFU1QvMLdi7mrYyux1aaGU25/jK7FMrzg8DDzA3TfP9Feetz1ocFwBVdaw9nwA+ztwP5rX+WTwKHK2qJ9r+Q8wF/oqNa7UD/jPA9vab/lcwt1J2zyrXtBIGV/POX+X71vbb8KuAFwf+KXZOSRLmFq0dqqr3Dhxa02NLMpFkQ9t+JXO/VzjEGl+ZXR2vOE/yqiSvObUN/AJwgDX+WayqZ4EjSf5Na7oOeJqVHNc58IuGG4F/YG4e9L+sdj3LqP/DwHHg/zP3E/k25uYy9wJfAv4OuKj1DXN3DX0Z+AIwudr1n2Fcb2Lun4b7gX3tceNaHxvws8Dn2rgOAP+1tf8E8CQwA/wv4PzWfkHbn2nHf2K1xzDEGK8BHullXG0Mn2+Pg6dyYq1/FlutO4Dp9nn8S+DClRyXK1klqVOrPUUjSRoTA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE79I1ROheW9h5AaAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# nb_steps represents the number of steps, you can try and change it\n",
        "dqn.fit(env, callbacks=[Render()], nb_steps=100, visualize=False, log_interval=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzKpwriO_ZVW"
      },
      "source": [
        "If you are running locally, uncomment this out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UklLLQIX_ZVW"
      },
      "outputs": [],
      "source": [
        "# #nb_steps represents the number of steps, you can try and change it\n",
        "# dqn.fit(env, nb_steps=100, visualize=True, verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCSn1qRP_ZVW"
      },
      "source": [
        "After training is done, we save the final weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5AneOXt_ZVW"
      },
      "outputs": [],
      "source": [
        "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlPszNOU_ZVW"
      },
      "source": [
        "Finally, evaluate our algorithm for 5 episodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvFvfOs6_ZVW"
      },
      "outputs": [],
      "source": [
        "dqn.test(env, nb_episodes=5, callbacks=[Render()], visualize=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7k9G_FK_ZVX"
      },
      "source": [
        "If you are running locally, uncomment this out!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep5n6hV5_ZVX"
      },
      "source": [
        "### SARSA & Expected SARSA\n",
        "\n",
        "Let's say that:\n",
        "\n",
        "$$X_t \\in \\{ v_t, \\hat{v_t} \\}$$\n",
        "\n",
        "where in expected SARSA:\n",
        "\n",
        "$$ v_t = r_t + \\gamma \\sum_a \\pi_t (s_{t+1}, a) Q_t (s_{t+1}, a)$$\n",
        "\n",
        "and in SARSA:\n",
        "\n",
        "$$ \\hat{v_t} = r_t + \\gamma Q_t (s_{t+1}, a_{t+1})$$\n",
        "\n",
        "Bias is represented by:\n",
        "\n",
        "$$Bias(s,a) = Q^{\\pi} (s,a) - E\\{X_t\\}$$\n",
        "\n",
        "Variance is denoted by:\n",
        "\n",
        "$$Var(s,a) = E\\{(X_t)^2\\} - (E\\{X_t\\})^2$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3saaKY4_ZVX"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Input, Dense, Activation, Flatten\n",
        "from rl.agents import SARSAAgent\n",
        "from rl.policy import BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6gfCjYy_ZVh"
      },
      "source": [
        "Get the environment and extract the number of actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlFLocMR_ZVh"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = 'CartPole-v0'\n",
        "\n",
        "env = gym.make(ENV_NAME)\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uztHWvzw_ZVh"
      },
      "source": [
        "Next, we build a very simple model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kx_It0cV_ZVh",
        "outputId": "146930d3-ddb2-4c3a-eaff-a15a5fac1d31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 1, 4)              0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 16)                80        \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 2)                 34        \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 658\n",
            "Trainable params: 658\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "inp = Input(shape=(1,) + env.observation_space.shape)\n",
        "x = Flatten()(inp)\n",
        "x = Dense(16)(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dense(16)(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dense(16)(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dense(nb_actions)(x)\n",
        "x = Activation('linear')(x)\n",
        "model = Model(inputs=inp, outputs=x)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eiw8MC7__ZVh"
      },
      "source": [
        "SARSA does not require a memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vX9xLlK_ZVi"
      },
      "outputs": [],
      "source": [
        "policy = BoltzmannQPolicy()\n",
        "sarsa = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=10, policy=policy)\n",
        "sarsa.compile(Adam(lr=1e-3), metrics=['mae'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vnti2l2_ZVi"
      },
      "source": [
        "Okay, now it's time to learn something! We visualize the training here for show, but this\n",
        "slows down training quite a lot. You can always safely abort the training prematurely using\n",
        "Ctrl + C."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX3RaTm1_ZVi",
        "outputId": "e48c2e43-d451-42a9-95a3-722971bbb870"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   18/10000 [..............................] - ETA: 26:49 - reward: 1.0000done, took 3.035 seconds\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc3b9d8b810>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAEpFJREFUeJzt3WGMnVed3/Hvr3ZIKNB1QqaWazt1dtcqylbFSafBEajKBrHrpLt1VtqipNUSoUhDpSCBFrWbbKUCUiPtvljSonaj9W6ymIoS0gCNG6XLpibSal+QMIAxdkyWAYxs14kHSAIUNa3Dvy/mGO5Oxp47c2c8vsffj3R1n+c853nuOfbVb54595y5qSokSf35G2vdAEnS6jDgJalTBrwkdcqAl6ROGfCS1CkDXpI6tWoBn2RXkmeTzCS5e7VeR5K0sKzGPPgk64C/At4BHAe+CNxeVc+s+ItJkha0Wnfw1wMzVfWtqvq/wEPA7lV6LUnSAtav0nU3A8cG9o8Dbzlb5SuvvLK2bdu2Sk2RpPFz9OhRvvvd72aUa6xWwC8qyRQwBXDVVVcxPT29Vk2RpAvO5OTkyNdYrSGaE8DWgf0treynqmpPVU1W1eTExMQqNUOSLl6rFfBfBLYnuTrJa4DbgH2r9FqSpAWsyhBNVZ1O8l7gc8A64MGqOrwaryVJWtiqjcFX1ePA46t1fUnSubmSVZI6ZcBLUqcMeEnqlAEvSZ0y4CWpUwa8JHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSp0b6yr4kR4EfAq8Ap6tqMskVwKeAbcBR4J1V9cJozZQkLdVK3MH/clXtqKrJtn83sL+qtgP7274k6TxbjSGa3cDetr0XuHUVXkOStIhRA76AP0/ypSRTrWxjVZ1s288BG0d8DUnSMow0Bg+8rapOJPnbwBNJvj54sKoqSS10YvuBMAVw1VVXjdgMSdJ8I93BV9WJ9nwK+CxwPfB8kk0A7fnUWc7dU1WTVTU5MTExSjMkSQtYdsAneV2SN5zZBn4FOATsA+5o1e4AHh21kZKkpRtliGYj8NkkZ67zX6rqz5J8EXg4yZ3Ad4B3jt5MSdJSLTvgq+pbwJsXKP8e8PZRGiVJGp0rWSWpUwa8JHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROLRrwSR5McirJoYGyK5I8keQb7fnyVp4kH00yk+RgkutWs/GSpLMb5g7+Y8CueWV3A/urajuwv+0D3Axsb48p4P6VaaYkaakWDfiq+gvg+/OKdwN72/Ze4NaB8o/XnC8AG5JsWqnGSpKGt9wx+I1VdbJtPwdsbNubgWMD9Y63sldJMpVkOsn07OzsMpshSTqbkT9kraoCahnn7amqyaqanJiYGLUZkqR5lhvwz58ZemnPp1r5CWDrQL0trUySdJ4tN+D3AXe07TuARwfK39Vm0+wEXhoYypEknUfrF6uQ5JPAjcCVSY4DHwR+D3g4yZ3Ad4B3tuqPA7cAM8CPgXevQpslSUNYNOCr6vazHHr7AnULuGvURkmSRudKVknqlAEvSZ0y4CWpUwa8JHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnVo04JM8mORUkkMDZR9KciLJgfa4ZeDYPUlmkjyb5FdXq+GSpHMb5g7+Y8CuBcrvq6od7fE4QJJrgNuAX2rn/GGSdSvVWEnS8BYN+Kr6C+D7Q15vN/BQVb1cVd8GZoDrR2ifJGmZRhmDf2+Sg20I5/JWthk4NlDneCt7lSRTSaaTTM/Ozo7QDEnSQpYb8PcDvwDsAE4Cf7DUC1TVnqqarKrJiYmJZTZDknQ2ywr4qnq+ql6pqp8Af8zPhmFOAFsHqm5pZZKk82xZAZ9k08DubwBnZtjsA25LcmmSq4HtwNOjNVGStBzrF6uQ5JPAjcCVSY4DHwRuTLIDKOAo8B6Aqjqc5GHgGeA0cFdVvbI6TZckncuiAV9Vty9Q/MA56t8L3DtKoyRJo3MlqyR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnVp0HrzUiy/tec+ryv7h1B+tQUuk88M7eEnqlAEvSZ0y4HVRW2jYRuqFAS9JnTLgJalTBrwuGs6Y0cXGgJekThnwuuj5Qat6ZcBLUqcMeEnq1KIBn2RrkieTPJPkcJL3tfIrkjyR5Bvt+fJWniQfTTKT5GCS61a7E9Kw/KBVF5Nh7uBPAx+oqmuAncBdSa4B7gb2V9V2YH/bB7gZ2N4eU8D9K95qSdKiFg34qjpZVV9u2z8EjgCbgd3A3lZtL3Br294NfLzmfAHYkGTTirdcWkF+0KoeLWkMPsk24FrgKWBjVZ1sh54DNrbtzcCxgdOOt7L515pKMp1kenZ2donNliQtZuiAT/J64NPA+6vqB4PHqqqAWsoLV9WeqpqsqsmJiYmlnCpJGsJQAZ/kEubC/RNV9ZlW/PyZoZf2fKqVnwC2Dpy+pZVJks6jYWbRBHgAOFJVHxk4tA+4o23fATw6UP6uNptmJ/DSwFCOtOacSaOLxTDf6PRW4LeAryU50Mp+F/g94OEkdwLfAd7Zjj0O3ALMAD8G3r2iLZYkDSVzw+dra3Jysqanp9e6GbrI+BV+upBNTk4yPT2dUa7hSlZJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8LpoLTRjxr9Jo54Y8JLUKQNekjplwEvzOEyjXhjwktQpA16SOmXA66Lm355Rzwx4SeqUAS8twA9a1QMDXpI6ZcBLUqcMeF30/KBVvTLgJalTw3zp9tYkTyZ5JsnhJO9r5R9KciLJgfa4ZeCce5LMJHk2ya+uZgckSQsb5g7+NPCBqroG2AncleSaduy+qtrRHo8DtGO3Ab8E7AL+MMm6VWi7tKqcSaNxt2jAV9XJqvpy2/4hcATYfI5TdgMPVdXLVfVtYAa4fiUaK0ka3pLG4JNsA64FnmpF701yMMmDSS5vZZuBYwOnHefcPxAkSatg6IBP8nrg08D7q+oHwP3ALwA7gJPAHyzlhZNMJZlOMj07O7uUU6UV50wa9WiogE9yCXPh/omq+gxAVT1fVa9U1U+AP+ZnwzAngK0Dp29pZX9NVe2pqsmqmpyYmBilD5KkBQwziybAA8CRqvrIQPmmgWq/ARxq2/uA25JcmuRqYDvw9Mo1WTp//KBV42z9EHXeCvwW8LUkB1rZ7wK3J9kBFHAUeA9AVR1O8jDwDHMzcO6qqldWuuGSpHNbNOCr6i+BLHDo8XOccy9w7wjtkiSNyJWsUuMHreqNAS9JnTLgJalTBry0CGfSaFwZ8JLUKQNekjplwEsDnEmjnhjwktQpA14agh+0ahwZ8NI8DtOoFwa8JHXKgJeG5DCNxo0BL0mdMuB1UUky1GPU8xe7jnQ+GPDSAibfs2etmyCNbJgv/JAuWv/9f03NKzH4NT68g5fO4tXhLo0XA15agg9+cHqtmyANbZgv3b4sydNJvprkcJIPt/KrkzyVZCbJp5K8ppVf2vZn2vFtq9sF6fz59b/jEI3GxzB38C8DN1XVm4EdwK4kO4HfB+6rql8EXgDubPXvBF5o5fe1etLYMcw17ob50u0CftR2L2mPAm4C/nkr3wt8CLgf2N22AR4B/mOStOtIY2NuJs1fD/kPr01TpGUZagw+ybokB4BTwBPAN4EXq+p0q3Ic2Ny2NwPHANrxl4A3rmSjJUmLGyrgq+qVqtoBbAGuB9406gsnmUoynWR6dnZ21MtJkuZZ0iyaqnoReBK4AdiQ5MwQzxbgRNs+AWwFaMd/DvjeAtfaU1WTVTU5MTGxzOZLks5mmFk0E0k2tO3XAu8AjjAX9L/Zqt0BPNq297V92vHPO/4uSeffMCtZNwF7k6xj7gfCw1X1WJJngIeS/DvgK8ADrf4DwH9OMgN8H7htFdotSVrEMLNoDgLXLlD+LebG4+eX/x/gn61I6yRJy+ZKVknqlAEvSZ0y4CWpU/65YF1UnNCli4l38JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6ZcBLUqcMeEnqlAEvSZ0y4CWpU8N86fZlSZ5O8tUkh5N8uJV/LMm3kxxojx2tPEk+mmQmycEk1612JyRJrzbM34N/Gbipqn6U5BLgL5P8j3bsX1XVI/Pq3wxsb4+3APe3Z0nSebToHXzN+VHbvaQ9zvWtCbuBj7fzvgBsSLJp9KZKkpZiqDH4JOuSHABOAU9U1VPt0L1tGOa+JJe2ss3AsYHTj7cySdJ5NFTAV9UrVbUD2AJcn+TvA/cAbwL+EXAF8DtLeeEkU0mmk0zPzs4usdmSpMUsaRZNVb0IPAnsqqqTbRjmZeBPgetbtRPA1oHTtrSy+dfaU1WTVTU5MTGxvNZLks5qmFk0E0k2tO3XAu8Avn5mXD1JgFuBQ+2UfcC72myancBLVXVyVVovSTqrYWbRbAL2JlnH3A+Eh6vqsSSfTzIBBDgA/MtW/3HgFmAG+DHw7pVvtiRpMYsGfFUdBK5doPyms9Qv4K7RmyZJGoUrWSWpUwa8JHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6RODR3wSdYl+UqSx9r+1UmeSjKT5FNJXtPKL237M+34ttVpuiTpXJZyB/8+4MjA/u8D91XVLwIvAHe28juBF1r5fa2eJOk8Gyrgk2wB/gnwJ20/wE3AI63KXuDWtr277dOOv73VlySdR+uHrPfvgX8NvKHtvxF4sapOt/3jwOa2vRk4BlBVp5O81Op/d/CCSaaAqbb7cpJDy+rBhe9K5vW9E732C/rtm/0aL383yVRV7VnuBRYN+CS/Bpyqqi8luXG5LzRfa/Se9hrTVTW5Ute+kPTat177Bf32zX6NnyTTtJxcjmHu4N8K/NMktwCXAX8L+A/AhiTr2138FuBEq38C2AocT7Ie+Dnge8ttoCRpeRYdg6+qe6pqS1VtA24DPl9V/wJ4EvjNVu0O4NG2va/t045/vqpqRVstSVrUKPPgfwf47SQzzI2xP9DKHwDe2Mp/G7h7iGst+1eQMdBr33rtF/TbN/s1fkbqW7y5lqQ+uZJVkjq15gGfZFeSZ9vK12GGcy4oSR5McmpwmmeSK5I8keQb7fnyVp4kH219PZjkurVr+bkl2ZrkySTPJDmc5H2tfKz7luSyJE8n+Wrr14dbeRcrs3tdcZ7kaJKvJTnQZpaM/XsRIMmGJI8k+XqSI0luWMl+rWnAJ1kH/CfgZuAa4PYk16xlm5bhY8CueWV3A/urajuwn599DnEzsL09poD7z1Mbl+M08IGqugbYCdzV/m/GvW8vAzdV1ZuBHcCuJDvpZ2V2zyvOf7mqdgxMiRz39yLMzUj8s6p6E/Bm5v7vVq5fVbVmD+AG4HMD+/cA96xlm5bZj23AoYH9Z4FNbXsT8Gzb/iPg9oXqXegP5mZJvaOnvgF/E/gy8BbmFsqsb+U/fV8CnwNuaNvrW72sddvP0p8tLRBuAh4D0kO/WhuPAlfOKxvr9yJzU8i/Pf/ffSX7tdZDND9d9doMrogdZxur6mTbfg7Y2LbHsr/t1/drgafooG9tGOMAcAp4AvgmQ67MBs6szL4QnVlx/pO2P/SKcy7sfgEU8OdJvtRWwcP4vxevBmaBP23Dan+S5HWsYL/WOuC7V3M/asd2qlKS1wOfBt5fVT8YPDaufauqV6pqB3N3vNcDb1rjJo0sAyvO17otq+RtVXUdc8MUdyX5x4MHx/S9uB64Dri/qq4F/jfzppWP2q+1Dvgzq17PGFwRO86eT7IJoD2fauVj1d8klzAX7p+oqs+04i76BlBVLzK3YO8G2srsdmihldlc4Cuzz6w4Pwo8xNwwzU9XnLc649gvAKrqRHs+BXyWuR/M4/5ePA4cr6qn2v4jzAX+ivVrrQP+i8D29kn/a5hbKbtvjdu0EgZX885f5fuu9mn4TuClgV/FLihJwtyitSNV9ZGBQ2PdtyQTSTa07dcy97nCEcZ8ZXZ1vOI8yeuSvOHMNvArwCHG/L1YVc8Bx5L8vVb0duAZVrJfF8AHDbcAf8XcOOi/Wev2LKP9nwROAv+PuZ/IdzI3lrkf+AbwP4ErWt0wN2vom8DXgMm1bv85+vU25n41PAgcaI9bxr1vwD8AvtL6dQj4t63854GngRngvwKXtvLL2v5MO/7za92HIfp4I/BYL/1qffhqexw+kxPj/l5sbd0BTLf3438DLl/JfrmSVZI6tdZDNJKkVWLAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUqf8PXaCdPKN6foIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# nb_steps represents the number of steps, you can try and change it\n",
        "dqn.fit(env, callbacks=[Render()], nb_steps=1000, log_interval=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTss8PNP_ZVi"
      },
      "source": [
        "If you are running locally, uncomment this out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKmlLj_2_ZVi"
      },
      "outputs": [],
      "source": [
        "# #nb_steps represents the number of steps, you can try and change it\n",
        "# sarsa.fit(env, nb_steps=10000, visualize=True, verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4caFZx5_ZVj"
      },
      "source": [
        "After training is done, we save the final weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EV5UaRjW_ZVl"
      },
      "outputs": [],
      "source": [
        "sarsa.save_weights('sarsa_{}_weights.h5f'.format(ENV_NAME), overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1mViMiq_ZVl"
      },
      "source": [
        "Finally, evaluate our algorithm for 5 episodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJu3dCHi_ZVm"
      },
      "outputs": [],
      "source": [
        "dqn.test(env, nb_episodes=5, callbacks=[Render()], visualize=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPqGRWGe_ZVm"
      },
      "source": [
        "If you are running locally, uncomment this out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6q29ZclB_ZVm"
      },
      "outputs": [],
      "source": [
        "# sarsa.test(env, nb_episodes=5, visualize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UWSpOwK_ZVm"
      },
      "source": [
        "## Deep Q-networks\n",
        "\n",
        "While our ordinary Q-network was able to barely perform as well as the Q-Table in a simple game environment, Deep $Q$-Networks are much more capable. In order to transform an ordinary Q-Network into a DQN we will be making the following improvements:\n",
        "+ Going from a single-layer network to a multi-layer convolutional network.\n",
        "+ Implementing Experience Replay, which will allow our network to train itself using stored memories from it’s experience.\n",
        "+ Utilizing a second “target” network, which we will use to compute target $Q$-values during our updates.\n",
        "\n",
        "<img src=\"https://github.com/ypeleg/keras_rl_tutorial/blob/master/images/deepq1.png?raw=1\" alt=\"\" style=\"width: 800px;\"/>\n",
        "\n",
        "\n",
        "See https://jaromiru.com/2016/09/27/lets-make-a-dqn-theory/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7688KCR_ZVn"
      },
      "source": [
        "### Convolutional Layers\n",
        "\n",
        "Since our agent is going to be learning to play video games, it has to be able to make sense of the game’s screen output in a way that is at least similar to how humans or other intelligent animals are able to. Instead of considering each pixel independently, convolutional layers allow us to consider regions of an image, and maintain spatial relationships between the objects on the screen as we send information up to higher levels of the network.\n",
        "\n",
        "### Experience Replay\n",
        "\n",
        "The second major addition to make DQNs work is Experience Replay.\n",
        "\n",
        "The problem with online learning is that the *samples arrive in order* they are experienced and as such are highly correlated. Because of this, our network will most likely overfit and fail to generalize properly.\n",
        "\n",
        "The key idea of **experience replay** is that we store these transitions in our memory and during each learning step, sample a random batch and perform a gradient descend on it.\n",
        "\n",
        "The Experience Replay buffer stores a fixed number of recent memories, and as new ones come in, old ones are removed. When the time comes to train, we simply draw a uniform batch of random memories from the buffer, and train our network with them.\n",
        "\n",
        "### Separate Target Network\n",
        "\n",
        "This second network is used to generate the $Q$-target values that will be used to compute the loss for every action during training.\n",
        "\n",
        "The issue is that at every step of training, the $Q$-network’s values shift, and if we are using a constantly shifting set of values to adjust our network values, then the value estimations can easily spiral out of control. The network can become destabilized by falling into feedback loops between the target and estimated $Q$-values. In order to mitigate that risk, the target network’s weights are fixed, and only periodically or slowly updated to the primary $Q$-networks values. In this way training can proceed in a more stable manner.\n",
        "\n",
        "Instead of updating the target network periodically and all at once, we will be updating it frequently, but slowly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGRjELyF_ZVn"
      },
      "source": [
        "While the DQN we have described above could learn ATARI games with enough training, getting the network to perform well on those games takes at least a day of training on a powerful machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pq8xKBdG_ZVn"
      },
      "source": [
        "### Installations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0g3xGDI_ZVn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.exists('keras_rl_tutorial'): os.system('git clone https://github.com/ypeleg/keras_rl_tutorial/')\n",
        "os.chdir('keras_rl_tutorial')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfJ8wiRJ_ZVo"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "keras.__version__\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()\n",
        "print('Running! \\nPlease dont interrupt this cell. It might cause serious issues..')\n",
        "!pip install gym keras-rl pyglet==1.2.4\n",
        "# !apt-get install -y cmake zlib1g-dev libjpeg-dev xvfb ffmpeg xorg-dev python-opengl libboost-all-dev libsdl2-dev swig\n",
        "!pip install 'gym[atari]'\n",
        "print('Done!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H3Weyl-_ZVo"
      },
      "source": [
        "### Imports"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NZ8RJ2h_ZVo"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "from IPython import display\n",
        "from PIL import Image\n",
        "\n",
        "from keras.callbacks import Callback\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute, Input\n",
        "from keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKu40l2L_ZVo"
      },
      "outputs": [],
      "source": [
        "INPUT_SHAPE = (84, 84)\n",
        "WINDOW_LENGTH = 4\n",
        "ENV_NAME = 'BreakoutDeterministic-v4'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65_ioKf0_ZVo"
      },
      "outputs": [],
      "source": [
        "class AtariProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3  # (height, width, channel)\n",
        "        img = Image.fromarray(observation)\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')  # resize and convert to grayscale\n",
        "        processed_observation = np.array(img)\n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')  # saves storage in experience memory\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        # We could perform this processing step in `process_observation`. In this case, however,\n",
        "        # we would need to store a `float32` array instead, which is 4x more memory intensive than\n",
        "        # an `uint8` array. This matters if we store 1M observations.\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8p75peL_ZVp"
      },
      "source": [
        "Get the environment and extract the number of actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxxY8yZq_ZVp"
      },
      "outputs": [],
      "source": [
        "env = gym.make(ENV_NAME)\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1Qs9bg1_ZVp"
      },
      "source": [
        "Next, we build our model. We use the same model that was described by Mnih et al. (2015)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9EnIODs_ZVp",
        "outputId": "1edc59a9-ba8d-4720-c2fb-62535ea266e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/user/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 4, 84, 84)         0         \n",
            "_________________________________________________________________\n",
            "permute_1 (Permute)          (None, 84, 84, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 20, 20, 32)        8224      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 20, 20, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 9, 9, 64)          32832     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 9, 9, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 4, 4, 64)          36928     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 2052      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 4)                 0         \n",
            "=================================================================\n",
            "Total params: 604,836\n",
            "Trainable params: 604,836\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
        "\n",
        "inp = Input(shape=input_shape)\n",
        "X = Permute((2, 3, 1))(inp)\n",
        "X = Convolution2D(32, (8, 8), strides=(4, 4))(X)\n",
        "X = Activation('relu')(X)\n",
        "X = Convolution2D(64, (4, 4), strides=(2, 2))(X)\n",
        "X = Activation('relu')(X)\n",
        "X = Convolution2D(64, (3, 3), strides=(2, 2))(X)\n",
        "X = Activation('relu')(X)\n",
        "X = Flatten()(X)\n",
        "X = Dense(512)(X)\n",
        "X = Activation('relu')(X)\n",
        "X = Dense(nb_actions)(X)\n",
        "X = Activation('linear')(X)\n",
        "model = Model(inputs=inp, outputs=X)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_vveTYR_ZVq"
      },
      "source": [
        "Finally, we configure and compile our agent. You can use every built-in Keras optimizer and even the metrics!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpjnKy_I_ZVq"
      },
      "outputs": [],
      "source": [
        "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
        "processor = AtariProcessor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3d-pmNp_ZVq"
      },
      "source": [
        "Select a policy. We use eps-greedy action selection, which means that a random action is selected with probability eps. We anneal eps from 1.0 to 0.1 over the course of 1M steps. This is done so that the agent initially explores the environment (high eps) and then gradually sticks to what it knows (low eps). We also set a dedicated eps value that is used during testing. Note that we set it to 0.05 so that the agent still performs some random actions. This ensures that the agent cannot get stuck."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmDwpb98_ZVq"
      },
      "outputs": [],
      "source": [
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05,\n",
        "                              nb_steps=1000000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8pNd_U0_ZVq"
      },
      "source": [
        "The trade-off between exploration and exploitation is difficult and an on-going research topic. If you want, you can experiment with the parameters or use a different policy. Another popular one is Boltzmann-style exploration:\n",
        "policy = BoltzmannQPolicy(tau=1.) Feel free to give it a try!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFRtRHQL_ZVr"
      },
      "outputs": [],
      "source": [
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,\n",
        "               processor=processor, nb_steps_warmup=50000, gamma=.99, target_model_update=10000,\n",
        "               train_interval=4, delta_clip=1.)\n",
        "dqn.compile(Adam(lr=.00025), metrics=['mae'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvefmWV3_ZVr"
      },
      "source": [
        "Okay, now it's time to learn something! We capture the interrupt exception so that training\n",
        "can be prematurely aborted. Notice that now you can use the built-in Keras callbacks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFzlcOhR_ZVr",
        "outputId": "a51df5dc-c50b-441d-a9bc-6b70c6fa7621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 1750000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            " 793/5000 [===>..........................] - ETA: 9s - reward: 0.0063done, took 1.842 seconds\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc5545cd0d0>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weights_filename = 'dqn_{}_weights.h5f'.format(ENV_NAME)\n",
        "checkpoint_weights_filename = 'dqn_' + ENV_NAME + '_weights_{step}.h5f'\n",
        "log_filename = 'dqn_{}_log.json'.format(ENV_NAME)\n",
        "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n",
        "callbacks += [FileLogger(log_filename, interval=100)]\n",
        "# dqn.fit(env, callbacks=callbacks, nb_steps=1750000, log_interval=10000)\n",
        "dqn.fit(env, callbacks=callbacks, nb_steps=1750000, log_interval=5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctMhuhTR_ZVr"
      },
      "source": [
        "After training is done, we save the final weights one more time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-H_cLtzE_ZVr"
      },
      "outputs": [],
      "source": [
        "dqn.save_weights(weights_filename, overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FKjf-Ug_ZVs"
      },
      "source": [
        "## Let's test it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgWoDJyK_ZVs"
      },
      "outputs": [],
      "source": [
        "weights_filename = 'dqn_{}_weights.h5f'.format(ENV_NAME)\n",
        "dqn.load_weights(weights_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zMJrTAj_ZVs"
      },
      "source": [
        "# Renderer\n",
        "for live animation in jupyter while running those command.\n",
        "in case you are running on your local device you can comment the bottom lines and and run full local environment! (It looks nicer :) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ohbziJ6_ZVs"
      },
      "outputs": [],
      "source": [
        "class Render(Callback):\n",
        "    def on_step_end(self, step, logs={}):\n",
        "        plt.clf()\n",
        "        plt.imshow(env.render(mode='rgb_array'))\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqTIrAV7_ZVs"
      },
      "source": [
        "Finally, evaluate our algorithm for 10 episodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_wOtvlG_ZVs",
        "outputId": "b5a20df8-0d0a-4b4f-ef77-4396197a16e6"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dqn.test(env, nb_episodes=10, visualize=False, callbacks=[Render()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Yj27akW_ZVt"
      },
      "source": [
        "And for those running locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFe9EXEZ_ZVt"
      },
      "outputs": [],
      "source": [
        "# dqn.test(env, nb_episodes=10, visualize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JydjlLQx_ZVt"
      },
      "source": [
        "And test it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0VdN1Vg_ZVt",
        "outputId": "386d25d3-b468-496b-c2ad-edecd02ba2cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 0.000, steps: 100000\n"
          ]
        }
      ],
      "source": [
        "weights_filename = 'dqn_{}_weights.h5f'.format(ENV_NAME)\n",
        "dqn.load_weights(weights_filename)\n",
        "dqn.test(env, nb_episodes=10, visualize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8RWZdgM_ZVu"
      },
      "source": [
        "### References\n",
        "- [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602), Mnih et al., 2013\n",
        "- [Human-level control through deep reinforcement learning](http://www.nature.com/nature/journal/v518/n7540/abs/nature14236.html), Mnih et al., 2015\n",
        "- [Deep Reinforcement Learning with Double Q-learning](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Applications_files/doubledqn.pdf), van Hasselt et al., 2015\n",
        "- [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581), Wang et al., 2016"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}